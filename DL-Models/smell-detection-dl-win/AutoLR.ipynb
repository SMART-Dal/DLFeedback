{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from matplotlib import pyplot\n",
    "from math import ceil\n",
    "from random import shuffle\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, TimeDistributed, Conv1D, MaxPooling1D, UpSampling1D\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.optimizers as opt\n",
    "import pandas as pd\n",
    "from transformers import RobertaTokenizer\n",
    "import os\n",
    "import transformers\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, y = make_classification(n_samples=1000, n_features=100, n_informative=10, n_redundant=90, random_state=1)\n",
    "# print(type(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basePath = r'/home/author/TagCoder/pythonNotebook'\n",
    "basePath = r'C:\\Users\\author\\Documents\\thesis\\pythonNotebook'\n",
    "positivePathSuffix = '/Positive'\n",
    "negativePathSuffix = '/Negative'\n",
    "tokenizerInPath = basePath + '\\\\tokenizerIn'\n",
    "tokenizerOutPath = basePath + '\\\\tokenizerOut'\n",
    "train_ratio = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "#tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "###try with code bert not just bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_max(arr, dim=\"width\", z=2):\n",
    "    mn = np.mean(arr, axis=0)\n",
    "    sd = np.std(arr, axis=0)\n",
    "    final_list = [x for x in arr if (x <= mn + z * sd)]  # upper outliers removed\n",
    "    rmn2 = len(arr) - len(final_list)\n",
    "    print('{} array size '.format(dim) + str(len(arr)))\n",
    "    print('min {} '.format(dim) + str(min(arr)))\n",
    "    print('max {} '.format(dim) + str(max(arr)))\n",
    "    print('mean {} '.format(dim) + str(np.nanmean(arr)))\n",
    "    print('standard deviation ' + str(np.std(arr)))\n",
    "    print('median {} '.format(dim) + str(np.nanmedian(arr)))\n",
    "    print('number of upper outliers removed ' + str(rmn2))\n",
    "    print('max {} excluding upper outliers '.format(dim) + str(max(final_list)))\n",
    "    return max(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_outlier_threshold(path, z, is_c2v):\n",
    "    print('Getting outlier threshold via inner method. The path passed is '+path)\n",
    "    lengths = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for f in files:\n",
    "            if f.startswith(\".\"):\n",
    "                continue\n",
    "            filepath = os.path.join(root, f)\n",
    "            with open(filepath, \"r\", errors='ignore') as file:\n",
    "                #print('Working with file'+ filepath)\n",
    "                for line in file:\n",
    "                    input_str = line.replace(\"\\t\", \" \")\n",
    "                    if is_c2v:\n",
    "                        np_arr = np.fromstring(input_str, dtype=np.float, sep=\" \")\n",
    "                    else:\n",
    "                        np_arr = np.fromstring(input_str, dtype=np.int32, sep=\" \")\n",
    "                    cur_width = len(np_arr)\n",
    "                    #print('cur_width: '+str(cur_width))\n",
    "                    lengths.append(cur_width)\n",
    "    \n",
    "    #print(' '.join(map(str, lengths)))\n",
    "    #print(compute_max(lengths,z=z))\n",
    "    return compute_max(lengths, z=z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len1 = _get_outlier_threshold((os.path.join(tokenizerOutPath,'ComplexMethod','Positive')),z=1,is_c2v = False)\n",
    "len2 = _get_outlier_threshold((os.path.join(tokenizerOutPath,'ComplexMethod','Negative')),z=1,is_c2v = False)\n",
    "\n",
    "if len1 > len2:\n",
    "    maxLength = len1 \n",
    "else:\n",
    "    maxLength = len2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smellList = ['Multifaceted']\n",
    "final_text = \"\"\n",
    "#maxLength = 0\n",
    "print(tokenizerInPath)\n",
    "for smell in smellList:\n",
    "    smellPath = os.path.join(tokenizerInPath, smell,'Positive',\"\")\n",
    "    #print(smellPath)\n",
    "    \n",
    "    for file in os.listdir(smellPath):\n",
    "        #print(os.path.basename(file))\n",
    "        with open(os.path.join(smellPath, file),\"r\",encoding='utf-8') as read_file:\n",
    "            try:\n",
    "                text = read_file.read()\n",
    "                tokenized_text = tokenizer.tokenize(text,padding = \"max_length\")\n",
    "                input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "                #print(len(input_ids))\n",
    "                # if len(input_ids) > maxLength:\n",
    "                #     maxLength = len(input_ids)\n",
    "                # modint = (len(input_ids)) % 512\n",
    "                # #print(modint)\n",
    "                # length = len(input_ids) - modint\n",
    "            \n",
    "                # input_ids = input_ids[0:length]\n",
    "                final_text += ' '.join(map(str, input_ids))+'\\n'\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                pass\n",
    "    #Path(os.path.join(tokenizerOutPath,smell,positivePathSuffix, 'tokenizer.tok')).touch(exist_ok=True)        \n",
    "    with open(os.path.abspath(os.path.join(tokenizerOutPath,smell,'Positive', 'tokenizer.tok')),'w',errors='ignore') as out_file:\n",
    "        #out_file.touch(exist_ok=True)\n",
    "        #print(final_text)\n",
    "        out_file.write(final_text)\n",
    "    \n",
    "smellPath = os.path.join(tokenizerInPath, smell,'Negative',\"\")\n",
    "print(smellPath)\n",
    "\n",
    "for file in os.listdir(smellPath):\n",
    "    #print(os.path.basename(file))\n",
    "    with open(os.path.join(smellPath, file),\"r\",encoding='utf-8') as read_file:\n",
    "        try:\n",
    "            text = read_file.read()\n",
    "            tokenized_text = tokenizer.tokenize(text)#,padding = \"max_length\")\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "            #print(len(input_ids))\n",
    "            # if len(input_ids) > maxLength:\n",
    "            #     maxLength = len(input_ids)\n",
    "            \n",
    "            # modint = (len(input_ids)) % 512\n",
    "            # #print(modint)\n",
    "            # length = len(input_ids) - modint\n",
    "           \n",
    "            # input_ids = input_ids[0:length]\n",
    "           \n",
    "            final_text += ' '.join(map(str, input_ids))+'\\n'\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "            pass\n",
    "#Path(os.path.join(tokenizerOutPath,smell,positivePathSuffix, 'tokenizer.tok')).touch(exist_ok=True)        \n",
    "with open(os.path.abspath(os.path.join(tokenizerOutPath,smell,'Negative', 'tokenizer.tok')),'w',errors='ignore') as out_file:\n",
    "    #out_file.touch(exist_ok=True)\n",
    "    #print(final_text)\n",
    "    out_file.write(final_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "\n",
    "num_lines_pos = sum(1 for line in open(os.path.join(tokenizerOutPath,smell,'Positive', 'tokenizer.tok'),\"r\"))\n",
    "with open(os.path.join(tokenizerOutPath,smell,'Positive', 'tokenizer.tok'),\"r\") as read_file:\n",
    "    #text = read_file.read()\n",
    "  \n",
    "    #text = text.replace('\\n', ' ')\n",
    "    #text = text.replace('\\r', ' ')\n",
    "    #print(text)\n",
    "    #posInput = np.fromstring(text, sep=\" \").tolist()\n",
    "    #print(len(posInput))\n",
    "    for line in read_file:\n",
    "        \n",
    "        if line == '\\n':\n",
    "            continue \n",
    "        arr = np.fromstring(line, dtype=np.int32, sep=\" \",count=maxLength)\n",
    "        arr_size = len(arr)\n",
    "        if arr_size <= maxLength:\n",
    "                    arr[arr_size:maxLength] = 0\n",
    "        X.append(arr)\n",
    "        Y = Y +[1]\n",
    "print(len(X))\n",
    "with open(os.path.join(tokenizerOutPath,smell,'Negative', 'tokenizer.tok'),\"r\") as read_file:\n",
    "    # text = read_file.read()\n",
    "  \n",
    "    # text = text.replace('\\n', ' ')\n",
    "    # text = text.replace('\\r', ' ')\n",
    "    # #print(text)\n",
    "    # negInput = np.fromstring(text, dtype=np.int32, sep=\" \").tolist()\n",
    "\n",
    "    for line in read_file:\n",
    "        if line == '\\n':\n",
    "            continue\n",
    "        arr = np.fromstring(line, dtype=np.int32, sep=\" \",count=maxLength)\n",
    "        arr_size = len(arr)\n",
    "        if arr_size <= maxLength:\n",
    "                    arr[arr_size:maxLength] = 0\n",
    "        X.append(arr)\n",
    "        Y = Y +[0]\n",
    "print(len(X))\n",
    "print((maxLength))\n",
    "\n",
    "X = np.asarray(X)\n",
    "\n",
    "print(type(X))\n",
    "n_inputs = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size= 1 - train_ratio, random_state=1)\n",
    "t = MinMaxScaler()\n",
    "t.fit(X_train)\n",
    "X_train = t.transform(X_train)\n",
    "X_test = t.transform(X_test)\n",
    "print(X_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visible = Input(shape=(n_inputs,))\n",
    "e = Dense(n_inputs*2)(visible)\n",
    "e = BatchNormalization()(e)\n",
    "e = LeakyReLU()(e)\n",
    "# encoder level 2\n",
    "e = Dense(n_inputs)(e)\n",
    "e = BatchNormalization()(e)\n",
    "e = LeakyReLU()(e)\n",
    "# bottleneck\n",
    "n_bottleneck = round(float(n_inputs) / 2.0)\n",
    "bottleneck = Dense(n_bottleneck)(e)\n",
    "# define decoder, level 1\n",
    "d = Dense(n_inputs)(bottleneck)\n",
    "d = BatchNormalization()(d)\n",
    "d = LeakyReLU()(d)\n",
    "# decoder level 2\n",
    "d = Dense(n_inputs*2)(d)\n",
    "d = BatchNormalization()(d)\n",
    "d = LeakyReLU()(d)\n",
    "# output layer\n",
    "output = Dense(n_inputs, activation='linear')(d)\n",
    "# define autoencoder model\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "# compile autoencoder model\n",
    "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
    "# plot_model(model, 'autoencoder_compress.png', show_shapes=True)\n",
    "# fit the autoencoder model to reconstruct input\n",
    "history = model.fit(X_train, X_train, epochs=200, batch_size=16, verbose=1, validation_data=(X_test,X_test),shuffle=True)\n",
    "# plot loss\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "# define an encoder model (without the decoder)\n",
    "encoder = Model(inputs=visible, outputs=bottleneck)\n",
    "# plot_model(encoder, 'encoder_compress.png', show_shapes=True)\n",
    "encoder.save('encoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = load_model('encoder.h5')\n",
    "X_train_encode = encoder.predict(X_train)\n",
    "X_test_encode = encoder.predict(X_test)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_encode, y_train)\n",
    "\n",
    "yhat = model.predict(X_test_encode)\n",
    "print(len(yhat))\n",
    "print(yhat)\n",
    "#yhat = yhat.pop()\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "print(acc)\n",
    "print(f1_score(y_test, yhat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yhat)\n",
    "#print(y_test[1])\n",
    "y_pred_bool = pd.cut(x=yhat.flatten(),bins=[0,0.4,1],labels=[0,1])\n",
    "#y_pred_bool =  torch.round(torch.tensor(yhat))\n",
    "#y_pred_bool = y_pred_bool.numpy()\n",
    "#print(collections.Counter(y_pred_bool.tolist()))\n",
    "plt.scatter(y_test,yhat)\n",
    "plt.show()\n",
    "#print(pd.DataFrame(y_pred_bool).describe())\n",
    "#print(\"Accuracy: %.2f%%\" % (scores[1]*100)) \n",
    "print(classification_report(y_test, yhat))\n",
    "acc = accuracy_score( yhat,y_test)\n",
    "# print(acc)\n",
    "print(f1_score(y_test, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.set(font_scale = 1.5)\n",
    "\n",
    "ax = sns.heatmap(\n",
    "    confusion_matrix(y_test, yhat), # confusion matrix 2D array \n",
    "    annot=True, # show numbers in the cells\n",
    "    fmt='d', # show numbers as integers\n",
    "    cbar=False, # don't show the color bar\n",
    "    cmap='flag', # customize color map\n",
    "    vmax=175 # to get better color contrast\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Predicted\", labelpad=20)\n",
    "ax.set_ylabel(\"Actual\", labelpad=20)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bfc6e335c128585d36af7a059510d5d913fec349af665acf4002268031b2667f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
