{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import transformers\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from random import shuffle\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, TimeDistributed, Conv1D, MaxPooling1D, UpSampling1D\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.optimizers as opt\n",
    "import pandas as pd\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, GPT2Tokenizer\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basePath = '/home/author/TagCoder/pythonNotebook'\n",
    "basePath = r'C:\\Users\\author\\Documents\\thesis\\pythonNotebook'\n",
    "positivePathSuffix = '/Positive'\n",
    "negativePathSuffix = '/Negative'\n",
    "tokenizerInPath = basePath + '\\\\tokenizerIn'\n",
    "tokenizerOutPath = r'C:\\Users\\author\\Documents\\thesis\\trial2\\tokenizer_out'\n",
    "train_ratio = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d9534fd9de467fba03c56c6b7fdf0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\author\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\huggingface_hub\\file_download.py:129: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\author\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "429d6bb789e047a8a546c7da738dbab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbcec3c2210c4226aac78b510147beaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "#tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\n",
    "#tokenizer = transformers.T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\author\\Documents\\thesis\\pythonNotebook\\tokenizerIn\n",
      "C:\\Users\\author\\Documents\\thesis\\pythonNotebook\\tokenizerIn\\ComplexMethod\\Negative\\\n"
     ]
    }
   ],
   "source": [
    "smellList = ['ComplexMethod']\n",
    "final_text = \"\"\n",
    "print(tokenizerInPath)\n",
    "for smell in smellList:\n",
    "    smellPath = os.path.join(tokenizerInPath, smell,'Positive',\"\")\n",
    "    #print(smellPath)\n",
    "    \n",
    "    for file in os.listdir(smellPath):\n",
    "        #print(os.path.basename(file))\n",
    "        with open(os.path.join(smellPath, file),\"r\",encoding='utf-8') as read_file:\n",
    "            try:\n",
    "                text = read_file.read()\n",
    "                tokenized_text = tokenizer.tokenize(text)#,padding = \"max_length\")\n",
    "                input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "                # modint = (len(input_ids)) % 512\n",
    "                # #print(modint)\n",
    "                # length = len(input_ids) - modint\n",
    "            \n",
    "                # input_ids = input_ids[0:length]\n",
    "                final_text += ' '.join(map(str, input_ids))+'\\n'\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                pass\n",
    "    #Path(os.path.join(tokenizerOutPath,smell,positivePathSuffix, 'tokenizer.tok')).touch(exist_ok=True)        \n",
    "    with open(os.path.abspath(os.path.join(tokenizerOutPath,smell,'1d','Positive', 'tokenizer.tok')),'w',errors='ignore') as out_file:\n",
    "        #out_file.touch(exist_ok=True)\n",
    "        #print(final_text)\n",
    "        out_file.write(final_text)\n",
    "    \n",
    "smellPath = os.path.join(tokenizerInPath, smell,'Negative',\"\")\n",
    "print(smellPath)\n",
    "\n",
    "for file in os.listdir(smellPath):\n",
    "    #print(os.path.basename(file))\n",
    "    with open(os.path.join(smellPath, file),\"r\",encoding='utf-8') as read_file:\n",
    "        try:\n",
    "            text = read_file.read()\n",
    "            tokenized_text = tokenizer.tokenize(text)#,padding = \"max_length\")\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "            # modint = (len(input_ids)) % 512\n",
    "            # #print(modint)\n",
    "            # length = len(input_ids) - modint\n",
    "           \n",
    "            # input_ids = input_ids[0:length]\n",
    "           \n",
    "            final_text += ' '.join(map(str, input_ids))+'\\n'\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "            pass\n",
    "#Path(os.path.join(tokenizerOutPath,smell,positivePathSuffix, 'tokenizer.tok')).touch(exist_ok=True)        \n",
    "with open(os.path.abspath(os.path.join(tokenizerOutPath,smell,'1d','Negative', 'tokenizer.tok')),'w',errors='ignore') as out_file:\n",
    "    #out_file.touch(exist_ok=True)\n",
    "    #print(final_text)\n",
    "    out_file.write(final_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\author\\\\Documents\\\\thesis\\\\trial2\\\\tokenizer_out\\\\ComplexMethod\\\\Positive\\\\tokenizer.tok'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m posInput \u001b[39m=\u001b[39m []\n\u001b[1;32m----> 2\u001b[0m num_lines_pos \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(\u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m \u001b[39mopen\u001b[39;49m(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(tokenizerOutPath,smell,\u001b[39m'\u001b[39;49m\u001b[39mPositive\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtokenizer.tok\u001b[39;49m\u001b[39m'\u001b[39;49m),\u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m      3\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(tokenizerOutPath,smell,\u001b[39m'\u001b[39m\u001b[39mPositive\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtokenizer.tok\u001b[39m\u001b[39m'\u001b[39m),\u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m read_file:\n\u001b[0;32m      4\u001b[0m     text \u001b[39m=\u001b[39m read_file\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\author\\\\Documents\\\\thesis\\\\trial2\\\\tokenizer_out\\\\ComplexMethod\\\\Positive\\\\tokenizer.tok'"
     ]
    }
   ],
   "source": [
    "posInput = []\n",
    "num_lines_pos = sum(1 for line in open(os.path.join(tokenizerOutPath,smell,'Positive', 'tokenizer.tok'),\"r\"))\n",
    "with open(os.path.join(tokenizerOutPath,smell,'Positive', 'tokenizer.tok'),\"r\") as read_file:\n",
    "    text = read_file.read()\n",
    "  \n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('\\r', ' ')\n",
    "    #print(text)\n",
    "    posInput = np.fromstring(text, sep=\" \").tolist()\n",
    "    print(len(posInput))\n",
    "    # for line in read_file:\n",
    "    #     if line == '\\n':\n",
    "    #         continue\n",
    "    #     arr = np.fromstring(line, dtype=np.int32, sep=\" \").tolist()\n",
    "    #     posInput.append(arr)\n",
    "\n",
    "negInput = []\n",
    "num_lines_neg = sum(1 for line in open(os.path.join(tokenizerOutPath,smell,'Negative', 'tokenizer.tok'),\"r\"))\n",
    "\n",
    "with open(os.path.join(tokenizerOutPath,smell,'Negative', 'tokenizer.tok'),\"r\") as read_file:\n",
    "    text = read_file.read()\n",
    "  \n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('\\r', ' ')\n",
    "    #print(text)\n",
    "    negInput = np.fromstring(text, dtype=np.int32, sep=\" \").tolist()\n",
    "\n",
    "    # for line in read_file:\n",
    "    #     if line == '\\n':\n",
    "    #         continue\n",
    "    #     arr = np.fromstring(line, dtype=np.int32, sep=\" \").tolist()\n",
    "    #     negInput.append(arr)\n",
    "\n",
    "num_lines_all =  num_lines_pos if num_lines_pos < num_lines_neg else num_lines_pos\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posInputLen = len(posInput)\n",
    "negInputLen = len(negInput)\n",
    "print(str(posInputLen)+\"  \"+str(negInputLen))\n",
    "train_data = []\n",
    "test_data = []\n",
    "posSize = ceil(posInputLen*train_ratio) - ceil(posInputLen*train_ratio) % 512\n",
    "print(posSize)\n",
    "negSize = ceil(negInputLen*train_ratio) - ceil(negInputLen*train_ratio) % 512\n",
    "\n",
    "test_data.append(posInput[posSize+1:])\n",
    "test_data[0] = test_data[0][0:len(test_data[0]) - (len(test_data[0])%512)]\n",
    "test_data.append(negInput[negSize+1:])\n",
    "test_data[1] = test_data[1][0:len(test_data[1]) - (len(test_data[1])%512)]\n",
    "\n",
    "test_data_flattened = list(chain.from_iterable(test_data))\n",
    "test_data_np = np.array(test_data_flattened)\n",
    "\n",
    "test_label = np.empty(shape=[len(test_data_np)], dtype=np.float32)\n",
    "print(len(test_label))\n",
    "test_label[0:posSize] = 1.0\n",
    "test_label[posSize+1:] = 0.0\n",
    "\n",
    "total_train_data = 0\n",
    "train_data.append(posInput[0:posSize])\n",
    "train_data[0] = train_data[0][0:len(train_data[0]) - (len(train_data[0])%512)]\n",
    "total_train_data += len(train_data[0])\n",
    "train_data.append(negInput[0:negSize])\n",
    "train_data[1] = train_data[1][0:len(train_data[1]) - (len(train_data[1])%512)]\n",
    "total_train_data += len(train_data[1])\n",
    "print('total_train_data'+str(total_train_data))\n",
    "train_data_flattened = list(chain.from_iterable(train_data))\n",
    "#shuffle(train_data_flattened)\n",
    "#print(train_data[1])\n",
    "train_data_np = np.array(train_data_flattened)\n",
    "train_data_np = train_data_np.reshape(-1,512,1)\n",
    "shuffle(train_data_np)\n",
    "print('arr shape')\n",
    "print(train_data_np.shape)\n",
    "test_data_np = test_data_np.reshape(len(test_label),1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder_lstm(train_data, test_data_np, smell, layers=1, encoding_dimension=8, no_of_epochs=10, with_bottleneck=True, is_final=False):\n",
    "    \n",
    "    encoding_dim = encoding_dimension\n",
    "    input_layer = Input(shape=(512, 1))\n",
    "    # input_layer = BatchNormalization()(input_layer)\n",
    "    no_of_layers = layers\n",
    "    prev_layer = input_layer\n",
    "    \n",
    "    for i in range(no_of_layers):\n",
    "        encoder = LSTM(int(encoding_dim / pow(2, i)),\n",
    "                        #activation=\"relu\",\n",
    "                       return_sequences=True,\n",
    "                       recurrent_dropout=0.1,\n",
    "                       dropout=0.1)(prev_layer)\n",
    "        prev_layer = encoder \n",
    "    \n",
    "    if with_bottleneck:\n",
    "        prev_layer = LSTM(int(encoding_dim / pow(2, no_of_layers + 1)),\n",
    "                         #activation=\"relu\",\n",
    "                          return_sequences=True,\n",
    "                          recurrent_dropout=0.1,\n",
    "                          dropout=0.1)(prev_layer)\n",
    "    for j in range(no_of_layers - 1, -1, -1):\n",
    "        decoder = LSTM(int(encoding_dim / pow(2, j)),\n",
    "                        #activation='relu',\n",
    "                       return_sequences=True,\n",
    "                       recurrent_dropout=0.1,\n",
    "                       dropout=0.1)(prev_layer)\n",
    "        prev_layer = decoder\n",
    "    prev_layer = TimeDistributed(Dense(1))(prev_layer)\n",
    "    autoencoder = Model(inputs=input_layer, outputs=prev_layer)\n",
    "\n",
    "    autoencoder.compile(optimizer='adam',\n",
    "                        loss='mean_squared_error',\n",
    "                        metrics=['accuracy'])\n",
    "    autoencoder.summary()   \n",
    "\n",
    "\n",
    "    batch_sizes = [32, 64]\n",
    "    b_size = int(len(train_data) / 512)\n",
    "    if b_size > len(batch_sizes) - 1:\n",
    "        b_size = len(batch_sizes) - 1\n",
    "    history = autoencoder.fit(train_data,\n",
    "                              train_data,\n",
    "                              epochs=no_of_epochs,\n",
    "                              batch_size=batch_sizes[b_size],\n",
    "                              verbose=1,\n",
    "                              validation_split=0.2,\n",
    "                              shuffle=True).history\n",
    "    \n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "    predictions = autoencoder.predict(test_data_np)\n",
    "    predictions = predictions.reshape(predictions.shape[0], predictions.shape[1])\n",
    "    test_data_np = test_data_np.reshape(test_data_np.shape[0], test_data_np.shape[1])\n",
    "    mse = np.mean(np.power(test_data_np - predictions, 2), axis=1)\n",
    "    error_df = pd.DataFrame({'Reconstruction_error': mse,\n",
    "                             'True_class': test_label})\n",
    "    print(error_df.describe())\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [1,2]\n",
    "encoding_dim = [8]\n",
    "epochs = 50\n",
    "cur_iter = 1\n",
    "skip_iter = 2\n",
    "for layer in layers:\n",
    "        for bottleneck in [True]:\n",
    "            for encoding in encoding_dim:\n",
    "                if cur_iter <= skip_iter:\n",
    "                    cur_iter += 1\n",
    "                    continue\n",
    "                cur_iter += 1\n",
    "                autoencoder_lstm(train_data_np,test_data_np, smell, layers=layer,encoding_dimension=encoding,no_of_epochs=epochs, with_bottleneck=bottleneck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.-1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bfc6e335c128585d36af7a059510d5d913fec349af665acf4002268031b2667f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
