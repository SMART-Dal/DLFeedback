{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import transformers\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from random import shuffle\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, TimeDistributed, Conv1D, MaxPooling1D, UpSampling1D\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.optimizers as opt\n",
    "import pandas as pd\n",
    "from transformers import RobertaTokenizer, BertTokenizerFast\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basePath = '/home/author/TagCoder/pythonNotebook'\n",
    "basePath = r'C:\\Users\\author\\Documents\\thesis\\pythonNotebook'\n",
    "positivePathSuffix = '/Positive'\n",
    "negativePathSuffix = '/Negative'\n",
    "tokenizerInPath = basePath + '\\\\tokenizerIn'\n",
    "tokenizerOutPath = basePath + '\\\\tokenizerOut'\n",
    "train_ratio = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\author\\Documents\\thesis\\pythonNotebook\\tokenizerIn\n",
      "C:\\Users\\author\\Documents\\thesis\\pythonNotebook\\tokenizerIn\\ComplexMethod\\Negative\\\n"
     ]
    }
   ],
   "source": [
    "tokenized_methods = []\n",
    "y_pos = []\n",
    "tokenized_methods_neg = []\n",
    "smellList = ['ComplexMethod']\n",
    "final_text = \"\"\n",
    "print(tokenizerInPath)\n",
    "for smell in smellList:\n",
    "    smellPath = os.path.join(tokenizerInPath, smell,'Positive',\"\")\n",
    "    #print(smellPath)\n",
    "    \n",
    "    for file in os.listdir(smellPath):\n",
    "        #print(os.path.basename(file))\n",
    "        with open(os.path.join(smellPath, file),\"r\",encoding=\"utf8\") as read_file:\n",
    "           \n",
    "            text = read_file.read()\n",
    "            tokenized_method = tokenizer(text, truncation=True, padding=True)\n",
    "            tokenized_methods.append(tokenized_method)\n",
    "            y_pos.append(1)\n",
    "    #Path(os.path.join(tokenizerOutPath,smell,positivePathSuffix, 'tokenizer.tok')).touch(exist_ok=True)        \n",
    "    # with open(os.path.abspath(os.path.join(tokenizerOutPath,smell,'Positive', 'tokenizer.tok')),'w',errors='ignore') as out_file:\n",
    "    #     #out_file.touch(exist_ok=True)\n",
    "    #     #print(final_text)\n",
    "    #     out_file.write(final_text)\n",
    "    \n",
    "smellPath = os.path.join(tokenizerInPath, smell,'Negative',\"\")\n",
    "print(smellPath)\n",
    "\n",
    "for file in os.listdir(smellPath):\n",
    "    #print(os.path.basename(file))\n",
    "    with open(os.path.join(smellPath, file),\"r\",encoding=\"utf8\") as read_file:\n",
    "        text = read_file.read()\n",
    "        tokenized_method = tokenizer(text, truncation=True, padding=True, max_length=512)\n",
    "        tokenized_methods.append(tokenized_method)\n",
    "        y_pos.append(0)\n",
    "#Path(os.path.join(tokenizerOutPath,smell,positivePathSuffix, 'tokenizer.tok')).touch(exist_ok=True)        \n",
    "# with open(os.path.abspath(os.path.join(tokenizerOutPath,smell,'Negative', 'tokenizer.tok')),'w',errors='ignore') as out_file:\n",
    "#     #out_file.touch(exist_ok=True)\n",
    "#     #print(final_text)\n",
    "#     out_file.write(final_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_methods = tokenized_methods[:25]\n",
    "tokenized_methods_neg = tokenized_methods_neg[:25]\n",
    "encoded_methods = []\n",
    "\n",
    "for tokenized_method in tokenized_methods:\n",
    "    encoded_method = model(**tokenized_method).last_hidden_state\n",
    "    encoded_methods.append(encoded_method)\n",
    "\n",
    "for tokenized_method in tokenized_methods_neg:\n",
    "    encoded_method = model(**tokenized_method).last_hidden_state\n",
    "    encoded_methods.append(encoded_method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tokenized_methods, y_pos, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'transformers.tokenization_utils_base.BatchEncoding'>\"}), (<class 'list'> containing values of types {\"<class 'int'>\"})",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39m# Compile the model\u001b[39;00m\n\u001b[0;32m     17\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 19\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\data_adapter.py:1081\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1078\u001b[0m adapter_cls \u001b[39m=\u001b[39m [\u001b[39mcls\u001b[39m \u001b[39mfor\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m ALL_ADAPTER_CLS \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcan_handle(x, y)]\n\u001b[0;32m   1079\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m adapter_cls:\n\u001b[0;32m   1080\u001b[0m     \u001b[39m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[1;32m-> 1081\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1082\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFailed to find data adapter that can handle input: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1083\u001b[0m             _type_name(x), _type_name(y)\n\u001b[0;32m   1084\u001b[0m         )\n\u001b[0;32m   1085\u001b[0m     )\n\u001b[0;32m   1086\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(adapter_cls) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1087\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1088\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mData adapters should be mutually exclusive for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1089\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhandling inputs. Found multiple adapters \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m to handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1090\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(adapter_cls, _type_name(x), _type_name(y))\n\u001b[0;32m   1091\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'transformers.tokenization_utils_base.BatchEncoding'>\"}), (<class 'list'> containing values of types {\"<class 'int'>\"})"
     ]
    }
   ],
   "source": [
    "#input_shape = encoded_methods.shape[1:]\n",
    "# Define the input layer\n",
    "inputs = Input(shape=(512,1))\n",
    "\n",
    "# Define the LSTM layers\n",
    "lstm1 = LSTM(64, return_sequences=True)(inputs)\n",
    "lstm2 = LSTM(64, return_sequences=True)(lstm1)\n",
    "lstm3 = LSTM(64, return_sequences=True)(lstm2)\n",
    "\n",
    "# Define the output layer\n",
    "output = TimeDistributed(Dense(1, activation='sigmoid'))(lstm3)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder_lstm(train_data, test_data_np, smell, layers=1, encoding_dimension=8, no_of_epochs=10, with_bottleneck=True, is_final=False):\n",
    "    \n",
    "    encoding_dim = encoding_dimension\n",
    "    input_layer = Input(shape=(512, 1))\n",
    "    # input_layer = BatchNormalization()(input_layer)\n",
    "    no_of_layers = layers\n",
    "    prev_layer = input_layer\n",
    "    \n",
    "    for i in range(no_of_layers):\n",
    "        encoder = LSTM(int(encoding_dim / pow(2, i)),\n",
    "                        #activation=\"relu\",\n",
    "                       return_sequences=True,\n",
    "                       recurrent_dropout=0.1,\n",
    "                       dropout=0.1)(prev_layer)\n",
    "        prev_layer = encoder \n",
    "    \n",
    "    if with_bottleneck:\n",
    "        prev_layer = LSTM(int(encoding_dim / pow(2, no_of_layers + 1)),\n",
    "                         #activation=\"relu\",\n",
    "                          return_sequences=True,\n",
    "                          recurrent_dropout=0.1,\n",
    "                          dropout=0.1)(prev_layer)\n",
    "    for j in range(no_of_layers - 1, -1, -1):\n",
    "        decoder = LSTM(int(encoding_dim / pow(2, j)),\n",
    "                        #activation='relu',\n",
    "                       return_sequences=True,\n",
    "                       recurrent_dropout=0.1,\n",
    "                       dropout=0.1)(prev_layer)\n",
    "        prev_layer = decoder\n",
    "    prev_layer = TimeDistributed(Dense(1))(prev_layer)\n",
    "    autoencoder = Model(inputs=input_layer, outputs=prev_layer)\n",
    "\n",
    "    autoencoder.compile(optimizer='adam',\n",
    "                        loss='mean_squared_error',\n",
    "                        metrics=['accuracy'])\n",
    "    autoencoder.summary()   \n",
    "\n",
    "\n",
    "    batch_sizes = [32, 64]\n",
    "    b_size = int(len(train_data) / 512)\n",
    "    if b_size > len(batch_sizes) - 1:\n",
    "        b_size = len(batch_sizes) - 1\n",
    "    history = autoencoder.fit(train_data,\n",
    "                              train_data,\n",
    "                              epochs=no_of_epochs,\n",
    "                              batch_size=batch_sizes[b_size],\n",
    "                              verbose=1,\n",
    "                              validation_split=0.2,\n",
    "                              shuffle=True).history\n",
    "    \n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "    predictions = autoencoder.predict(test_data_np)\n",
    "    predictions = predictions.reshape(predictions.shape[0], predictions.shape[1])\n",
    "    test_data_np = test_data_np.reshape(test_data_np.shape[0], test_data_np.shape[1])\n",
    "    mse = np.mean(np.power(test_data_np - predictions, 2), axis=1)\n",
    "    error_df = pd.DataFrame({'Reconstruction_error': mse,\n",
    "                             'True_class': test_label})\n",
    "    print(error_df.describe())\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [1,2]\n",
    "encoding_dim = [8, 16, 32]\n",
    "epochs = 100\n",
    "cur_iter = 1\n",
    "skip_iter = 2\n",
    "for layer in layers:\n",
    "        for bottleneck in [True]:\n",
    "            for encoding in encoding_dim:\n",
    "                if cur_iter <= skip_iter:\n",
    "                    cur_iter += 1\n",
    "                    continue\n",
    "                cur_iter += 1\n",
    "                autoencoder_lstm(train_data_np,test_data_np, smell, layers=layer,encoding_dimension=encoding,no_of_epochs=epochs, with_bottleneck=bottleneck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.-1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bfc6e335c128585d36af7a059510d5d913fec349af665acf4002268031b2667f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
