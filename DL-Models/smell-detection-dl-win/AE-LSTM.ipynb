{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pydot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from matplotlib import pyplot\n",
    "from math import ceil\n",
    "from random import shuffle\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, TimeDistributed, Conv1D, MaxPooling1D, UpSampling1D\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.optimizers as opt\n",
    "import pandas as pd\n",
    "from transformers import RobertaTokenizer\n",
    "import os\n",
    "import transformers\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, y = make_classification(n_samples=1000, n_features=100, n_informative=10, n_redundant=90, random_state=1)\n",
    "# print(type(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basePath = r'/home/author/TagCoder/pythonNotebook'\n",
    "basePath = r'C:\\Users\\author\\Documents\\thesis\\pythonNotebook'\n",
    "positivePathSuffix = '\\\\Positive'\n",
    "negativePathSuffix = '\\\\Negative'\n",
    "tokenizerInPath = basePath + '\\\\tokenizerIn'\n",
    "tokenizerOutPath = basePath + '\\\\tokenizerOut'\n",
    "train_ratio = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "#tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_max(arr, dim=\"width\", z=2):\n",
    "    mn = np.mean(arr, axis=0)\n",
    "    sd = np.std(arr, axis=0)\n",
    "    final_list = [x for x in arr if (x <= mn + z * sd)]  # upper outliers removed\n",
    "    rmn2 = len(arr) - len(final_list)\n",
    "    print('{} array size '.format(dim) + str(len(arr)))\n",
    "    print('min {} '.format(dim) + str(min(arr)))\n",
    "    print('max {} '.format(dim) + str(max(arr)))\n",
    "    print('mean {} '.format(dim) + str(np.nanmean(arr)))\n",
    "    print('standard deviation ' + str(np.std(arr)))\n",
    "    print('median {} '.format(dim) + str(np.nanmedian(arr)))\n",
    "    print('number of upper outliers removed ' + str(rmn2))\n",
    "    print('max {} excluding upper outliers '.format(dim) + str(max(final_list)))\n",
    "    return max(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_outlier_threshold(path, z, is_c2v):\n",
    "    print('Getting outlier threshold via inner method. The path passed is '+path)\n",
    "    lengths = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for f in files:\n",
    "            if f.startswith(\".\"):\n",
    "                continue\n",
    "            filepath = os.path.join(root, f)\n",
    "            with open(filepath, \"r\", errors='ignore') as file:\n",
    "                #print('Working with file'+ filepath)\n",
    "                for line in file:\n",
    "                    input_str = line.replace(\"\\t\", \" \")\n",
    "                    if is_c2v:\n",
    "                        np_arr = np.fromstring(input_str, dtype=np.float, sep=\" \")\n",
    "                    else:\n",
    "                        np_arr = np.fromstring(input_str, dtype=np.int32, sep=\" \")\n",
    "                    cur_width = len(np_arr)\n",
    "                    #print('cur_width: '+str(cur_width))\n",
    "                    lengths.append(cur_width)\n",
    "    \n",
    "    #print(' '.join(map(str, lengths)))\n",
    "    #print(compute_max(lengths,z=z))\n",
    "    return compute_max(lengths, z=z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len1 = _get_outlier_threshold((os.path.join(tokenizerOutPath,'ComplexMethod','Positive')),z=1,is_c2v = False)\n",
    "len2 = _get_outlier_threshold((os.path.join(tokenizerOutPath,'ComplexMethod','Negative')),z=1,is_c2v = False)\n",
    "\n",
    "if len1 > len2:\n",
    "    maxLength = len1 \n",
    "else:\n",
    "    maxLength = len2\n",
    "\n",
    "print(maxLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smellList = ['LongParameter']\n",
    "final_text = \"\"\n",
    "\n",
    "print(tokenizerInPath)\n",
    "for smell in smellList:\n",
    "    smellPath = os.path.join(tokenizerInPath, smell,'Positive',\"\")\n",
    "    #print(smellPath)\n",
    "    \n",
    "    for file in os.listdir(smellPath):\n",
    "        #print(os.path.basename(file))\n",
    "        with open(os.path.join(smellPath, file),\"r\",encoding='utf-8') as read_file:\n",
    "            try:\n",
    "                text = read_file.read()\n",
    "                tokenized_text = tokenizer.tokenize(text)\n",
    "                input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "                #print(len(input_ids))\n",
    "                # if len(input_ids) > maxLength:\n",
    "                #     maxLength = len(input_ids)\n",
    "                # modint = (len(input_ids)) % 512\n",
    "                # #print(modint)\n",
    "                # length = len(input_ids) - modint\n",
    "            \n",
    "                # input_ids = input_ids[0:length]\n",
    "                final_text += ' '.join(map(str, input_ids))+'\\n'\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                pass\n",
    "    #Path(os.path.join(tokenizerOutPath,smell,positivePathSuffix, 'tokenizer.tok')).touch(exist_ok=True)        \n",
    "    with open(os.path.abspath(os.path.join(tokenizerOutPath,smell,'1d','Positive', 'tokenizer.tok')),'w',errors='ignore') as out_file:\n",
    "        #out_file.touch(exist_ok=True)\n",
    "        #print(final_text)\n",
    "        out_file.write(final_text)\n",
    "    \n",
    "smellPath = os.path.join(tokenizerInPath, smell,'Negative',\"\")\n",
    "print(smellPath)\n",
    "\n",
    "for file in os.listdir(smellPath):\n",
    "    #print(os.path.basename(file))\n",
    "    with open(os.path.join(smellPath, file),\"r\",encoding='utf-8') as read_file:\n",
    "        try:\n",
    "            text = read_file.read()\n",
    "#            tokenized_text = tokenizer.tokenize(text,padding = \"max_length\")\n",
    "            tokenized_text = tokenizer.tokenize(text)\n",
    " \n",
    "            input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "            #print(len(input_ids))\n",
    "            # if len(input_ids) > maxLength:\n",
    "            #     maxLength = len(input_ids)\n",
    "            \n",
    "            # modint = (len(input_ids)) % 512\n",
    "            # #print(modint)\n",
    "            # length = len(input_ids) - modint\n",
    "           \n",
    "            # input_ids = input_ids[0:length]\n",
    "           \n",
    "            final_text += ' '.join(map(str, input_ids))+'\\n'\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "            pass\n",
    "#Path(os.path.join(tokenizerOutPath,smell,positivePathSuffix, 'tokenizer.tok')).touch(exist_ok=True)        \n",
    "with open(os.path.abspath(os.path.join(tokenizerOutPath,smell,'1d','Negative', 'tokenizer.tok')),'w',errors='ignore') as out_file:\n",
    "    #out_file.touch(exist_ok=True)\n",
    "    #print(final_text)\n",
    "    out_file.write(final_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "\n",
    "num_lines_pos = sum(1 for line in open(os.path.join(tokenizerOutPath,smell,'Positive', 'tokenizer.tok'),\"r\"))\n",
    "with open(os.path.join(tokenizerOutPath,smell,'Positive', 'tokenizer.tok'),\"r\") as read_file:\n",
    "    #text = read_file.read()\n",
    "  \n",
    "    #text = text.replace('\\n', ' ')\n",
    "    #text = text.replace('\\r', ' ')\n",
    "    #print(text)\n",
    "    #posInput = np.fromstring(text, sep=\" \").tolist()\n",
    "    #print(len(posInput))\n",
    "    for line in read_file:\n",
    "        if line == '\\n':\n",
    "            continue \n",
    "        arr = np.fromstring(line, dtype=np.int32, sep=\" \",count=maxLength)\n",
    "        arr_size = len(arr)\n",
    "        if arr_size <= maxLength:\n",
    "                    arr[arr_size:maxLength] = 0\n",
    "        X.append(arr)\n",
    "        Y = Y +[1]\n",
    "\n",
    "with open(os.path.join(tokenizerOutPath,smell,'Negative', 'tokenizer.tok'),\"r\") as read_file:\n",
    "    # text = read_file.read()\n",
    "  \n",
    "    # text = text.replace('\\n', ' ')\n",
    "    # text = text.replace('\\r', ' ')\n",
    "    # #print(text)\n",
    "    # negInput = np.fromstring(text, dtype=np.int32, sep=\" \").tolist()\n",
    "\n",
    "    for line in read_file:\n",
    "        if line == '\\n':\n",
    "            continue\n",
    "        arr = np.fromstring(line, dtype=np.int32, sep=\" \",count=maxLength)\n",
    "        arr_size = len(arr)\n",
    "        if arr_size <= maxLength:\n",
    "                    arr[arr_size:maxLength] = 0\n",
    "        X.append(arr)\n",
    "        Y = Y +[0]\n",
    "print((maxLength))\n",
    "\n",
    "X = np.asarray(X)\n",
    "\n",
    "print(type(X))\n",
    "n_inputs = X.shape[1]\n",
    "X,Y = shuffle(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size= 1 - train_ratio, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1,maxLength,1)\n",
    "X_test = X_test.reshape(-1,maxLength,1)\n",
    "#shuffle(X_train,y_train)\n",
    "print('arr shape')\n",
    "print(X_train.shape)\n",
    "collections.Counter(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(maxLength,1))\n",
    "\n",
    "encoding_dim = 8\n",
    "no_of_layers = 1\n",
    "with_bottleneck = False\n",
    "\n",
    "prev_layer = input_layer\n",
    "\n",
    "for i in range(no_of_layers):\n",
    "        encoder = LSTM(int(encoding_dim / pow(2, i)),\n",
    "                        #activation=\"relu\",\n",
    "                       return_sequences=True,\n",
    "                       recurrent_dropout=0.1,\n",
    "                       dropout=0.1)(prev_layer)\n",
    "        prev_layer = encoder \n",
    "\n",
    "if with_bottleneck:\n",
    "        prev_layer = LSTM(int(encoding_dim / pow(2, no_of_layers + 1)),\n",
    "                         #activation=\"relu\",\n",
    "                          return_sequences=True,\n",
    "                          recurrent_dropout=0.1,\n",
    "                          dropout=0.1)(prev_layer)\n",
    "for j in range(no_of_layers - 1, -1, -1):\n",
    "        decoder = LSTM(int(encoding_dim / pow(2, j)),\n",
    "                        #activation='relu',\n",
    "                       return_sequences=True,\n",
    "                       recurrent_dropout=0.1,\n",
    "                       dropout=0.1)(prev_layer)\n",
    "        prev_layer = decoder\n",
    "prev_layer = TimeDistributed(Dense(1))(prev_layer)\n",
    "autoencoder = Model(inputs=input_layer, outputs=prev_layer)\n",
    "\n",
    "autoencoder.compile(optimizer='adam',\n",
    "                        loss='mean_squared_error',\n",
    "                        metrics=['mean_squared_error'])\n",
    "autoencoder.summary()   \n",
    "\n",
    "\n",
    "history = autoencoder.fit(X_train,\n",
    "                              X_train,\n",
    "                              epochs=20,\n",
    "                              batch_size=32,\n",
    "                              verbose=1,\n",
    "                              validation_split=0.2,\n",
    "                              shuffle=True).history\n",
    "\n",
    "autoencoder.save('encoder_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save('encoder_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = load_model('encoder_lstm.h5')\n",
    "#autoencoder = load_model('encoder_lstm_float.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential \n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Embedding, Dropout, GlobalAveragePooling1D, Flatten, SpatialDropout1D, Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encode = autoencoder.predict(X_train)\n",
    "X_test_encode = autoencoder.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 727\n",
    "embedding_dim = 8\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "#model2.add(Embedding(vocab_size,embedding_dim,input_length = maxLength))\n",
    "model2.add(Input(shape=(maxLength,1)))\n",
    "#model2.add(Embedding(vocab_size,embedding_dim,input_length = maxLength))\n",
    "model2.add(Bidirectional(LSTM(64,\n",
    "                              return_sequences = False)))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "model2.compile(loss = 'binary_crossentropy',\n",
    "               optimizer = 'adam',\n",
    "               metrics=['accuracy'])\n",
    "num_epochs = 30\n",
    "early_stop = EarlyStopping(monitor = 'val_loss',\n",
    "                           patience = 2)\n",
    "history = model2.fit(X_train_encode,\n",
    "                     y_train,\n",
    "                     epochs = num_epochs - 15,\n",
    "                     validation_data = (X_test_encode, y_test),\n",
    "                     #callbacks = [early_stop],\n",
    "                     verbose = 1)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model2.predict(X_test_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round(x, decimals=0):\n",
    "    b = 10**decimals\n",
    "    return torch.round(x*b)/b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#scores = model2.evaluate(X_test, y_test, verbose=1)\n",
    "print(yhat)\n",
    "#print(y_test[1])\n",
    "y_pred_bool = pd.cut(x=yhat.flatten(),bins=[0,0.36,1],labels=[0,1])\n",
    "#y_pred_bool =  torch.round(torch.tensor(yhat))\n",
    "#y_pred_bool = y_pred_bool.numpy()\n",
    "print(collections.Counter(y_pred_bool.tolist()))\n",
    "plt.scatter(y_test,yhat)\n",
    "plt.show()\n",
    "#print(pd.DataFrame(y_pred_bool).describe())\n",
    "#print(\"Accuracy: %.2f%%\" % (scores[1]*100)) \n",
    "print(classification_report(y_test, y_pred_bool))\n",
    "acc = accuracy_score( y_pred_bool,y_test)\n",
    "# print(acc)\n",
    "print(f1_score(y_test, y_pred_bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.set(font_scale = 1.5)\n",
    "\n",
    "ax = sns.heatmap(\n",
    "    confusion_matrix(y_test, y_pred_bool), # confusion matrix 2D array \n",
    "    annot=True, # show numbers in the cells\n",
    "    fmt='d', # show numbers as integers\n",
    "    cbar=False, # don't show the color bar\n",
    "    cmap='flag', # customize color map\n",
    "    vmax=175 # to get better color contrast\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Predicted\", labelpad=20)\n",
    "ax.set_ylabel(\"Actual\", labelpad=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bfc6e335c128585d36af7a059510d5d913fec349af665acf4002268031b2667f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
